{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transfer_Learning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MDw7YEQNcU8i","outputId":"ddcb4f50-5e71-404f-aa63-0b041c9ad099","executionInfo":{"status":"ok","timestamp":1654049917723,"user_tz":-330,"elapsed":18691,"user":{"displayName":"Saurabh Zinjad","userId":"09657915851787292245"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","source":["# Import Libraries"],"metadata":{"id":"WZcbAFuPiRbG"}},{"cell_type":"code","source":["import os\n","import cv2\n","import copy\n","import random\n","import argparse\n","import numpy as np\n","\n","import torch\n","import torch.optim as optim\n","\n","from network import MyNet\n","from torchsummary import summary\n","from dataset import CropData, LoadImages\n","from torch.utils.data import DataLoader, Dataset\n","\n","from torchvision.models import resnet50, resnet101, resnet18"],"metadata":{"id":"z-Gs7vasfLWc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TL: No weights"],"metadata":{"id":"Bzf_-lqIqpwq"}},{"cell_type":"code","source":["resnet_18 = resnet18(pretrained=False)\n","resnet_18 = torch.nn.Sequential(*list(resnet_18.children())[:-2])"],"metadata":{"id":"xLwVVutZeRXQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["IMG_PATH = \"//content/drive/MyDrive/Data-20220522T105527Z-002/Rye\""],"metadata":{"id":"z4DUHJp_h-eA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["crop_images_array = LoadImages(IMG_PATH, dsize=(224, 224)).load_images_into_array()\n","\n","crop_dataset = CropData(\n","    images=crop_images_array\n","    )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JS-7IqBEhn_b","outputId":"46316a34-7a43-4138-ae7f-91b9719568b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Images loaded into array:  (315, 224, 224, 3)\n"]}]},{"cell_type":"code","source":["crop_dataloader = DataLoader(crop_dataset, batch_size=32, shuffle=True)"],"metadata":{"id":"efg26f9Op_nl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = MyNet(3, 100, 3)"],"metadata":{"id":"VyAi7zVrearZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary(resnet_18, input_size=(3, 224, 224))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_dvWmIVIfqjT","outputId":"ec72c034-bc74-43a0-c724-a9da0c9898c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","       BatchNorm2d-2         [-1, 64, 112, 112]             128\n","              ReLU-3         [-1, 64, 112, 112]               0\n","         MaxPool2d-4           [-1, 64, 56, 56]               0\n","            Conv2d-5           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-6           [-1, 64, 56, 56]             128\n","              ReLU-7           [-1, 64, 56, 56]               0\n","            Conv2d-8           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-9           [-1, 64, 56, 56]             128\n","             ReLU-10           [-1, 64, 56, 56]               0\n","       BasicBlock-11           [-1, 64, 56, 56]               0\n","           Conv2d-12           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-13           [-1, 64, 56, 56]             128\n","             ReLU-14           [-1, 64, 56, 56]               0\n","           Conv2d-15           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-16           [-1, 64, 56, 56]             128\n","             ReLU-17           [-1, 64, 56, 56]               0\n","       BasicBlock-18           [-1, 64, 56, 56]               0\n","           Conv2d-19          [-1, 128, 28, 28]          73,728\n","      BatchNorm2d-20          [-1, 128, 28, 28]             256\n","             ReLU-21          [-1, 128, 28, 28]               0\n","           Conv2d-22          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-23          [-1, 128, 28, 28]             256\n","           Conv2d-24          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-25          [-1, 128, 28, 28]             256\n","             ReLU-26          [-1, 128, 28, 28]               0\n","       BasicBlock-27          [-1, 128, 28, 28]               0\n","           Conv2d-28          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-29          [-1, 128, 28, 28]             256\n","             ReLU-30          [-1, 128, 28, 28]               0\n","           Conv2d-31          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-32          [-1, 128, 28, 28]             256\n","             ReLU-33          [-1, 128, 28, 28]               0\n","       BasicBlock-34          [-1, 128, 28, 28]               0\n","           Conv2d-35          [-1, 256, 14, 14]         294,912\n","      BatchNorm2d-36          [-1, 256, 14, 14]             512\n","             ReLU-37          [-1, 256, 14, 14]               0\n","           Conv2d-38          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-39          [-1, 256, 14, 14]             512\n","           Conv2d-40          [-1, 256, 14, 14]          32,768\n","      BatchNorm2d-41          [-1, 256, 14, 14]             512\n","             ReLU-42          [-1, 256, 14, 14]               0\n","       BasicBlock-43          [-1, 256, 14, 14]               0\n","           Conv2d-44          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-45          [-1, 256, 14, 14]             512\n","             ReLU-46          [-1, 256, 14, 14]               0\n","           Conv2d-47          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-48          [-1, 256, 14, 14]             512\n","             ReLU-49          [-1, 256, 14, 14]               0\n","       BasicBlock-50          [-1, 256, 14, 14]               0\n","           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n","      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n","             ReLU-53            [-1, 512, 7, 7]               0\n","           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n","           Conv2d-56            [-1, 512, 7, 7]         131,072\n","      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n","             ReLU-58            [-1, 512, 7, 7]               0\n","       BasicBlock-59            [-1, 512, 7, 7]               0\n","           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n","             ReLU-62            [-1, 512, 7, 7]               0\n","           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n","             ReLU-65            [-1, 512, 7, 7]               0\n","       BasicBlock-66            [-1, 512, 7, 7]               0\n","================================================================\n","Total params: 11,176,512\n","Trainable params: 11,176,512\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 62.78\n","Params size (MB): 42.64\n","Estimated Total Size (MB): 105.99\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["summary(net, input_size=(3, 224, 224))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ep9Y4Tw_mCp7","outputId":"e598676e-01f1-4cc9-bff2-45c4755fac41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1        [-1, 100, 224, 224]           2,800\n","       BatchNorm2d-2        [-1, 100, 224, 224]             200\n","            Conv2d-3        [-1, 100, 224, 224]          90,100\n","       BatchNorm2d-4        [-1, 100, 224, 224]             200\n","            Conv2d-5        [-1, 100, 224, 224]          90,100\n","       BatchNorm2d-6        [-1, 100, 224, 224]             200\n","            Conv2d-7        [-1, 100, 224, 224]          10,100\n","       BatchNorm2d-8        [-1, 100, 224, 224]             200\n","================================================================\n","Total params: 193,900\n","Trainable params: 193,900\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 306.25\n","Params size (MB): 0.74\n","Estimated Total Size (MB): 307.56\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# model = MyNet(input_dim=3, nChannel=args.nChannel, nConv=args.nConv)\n","\n","model = resnet_18.train()\n","# device = \"cpu\"\n","model = resnet_18.to(device)\n","\n","# similarity loss definition\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# scribble loss definition\n","loss_fn_scr = torch.nn.CrossEntropyLoss()\n","\n","# continuity loss definition\n","loss_hpy = torch.nn.L1Loss(size_average = True)\n","loss_hpz = torch.nn.L1Loss(size_average = True)\n","\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","stepsize_sim = 0.5; stepsize_con = 1"],"metadata":{"id":"IO1NiGVvmAtc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7e26ebea-6689-4b81-8c6a-28dda35512cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n"]}]},{"cell_type":"code","source":["def train(data_loader, epochs, minLabels):\n","  for batch_idx in range(1, epochs+1):\n","    min_loss = 0.0\n","    for X in data_loader:\n","      X = X.to(device)\n","      optimizer.zero_grad()\n","      output = model( X )\n","      target = output.data\n","      output = output.permute( 0, 2, 3, 1).contiguous().view( -1, output.shape[1] )\n"," \n","      HPy_target = torch.zeros(target.shape[0], target.shape[2]-1, target.shape[3], target.shape[1]).to(device)\n","      HPz_target = torch.zeros(target.shape[0], target.shape[2], target.shape[3]-1, target.shape[1]).to(device)\n","\n","      outputHP = output.reshape( (target.shape[0], target.shape[2], target.shape[3], target.shape[1]) )\n","      HPy = outputHP[:, 1:, :, :] - outputHP[:, 0:-1, :, :]\n","      HPz = outputHP[:, :, 1:, :] - outputHP[:, :, 0:-1, :]\n","\n","      HPy = HPy.to(device); HPz = HPz.to(device)\n","\n","      lhpy = loss_hpy(HPy, HPy_target)\n","      lhpz = loss_hpz(HPz, HPz_target)\n","\n","      _, target = torch.max( output, 1 )\n","\n","      im_target = target.cpu().numpy()\n","      nLabels = len(np.unique(im_target))\n","            \n","      loss = stepsize_sim * loss_fn(output, target) + stepsize_con * (lhpy + lhpz)\n","                \n","      loss.backward()\n","      optimizer.step()\n","\n","      if loss < min_loss:\n","        min_loss = loss\n","        best_model_weights = copy.deepcopy(model.state_dict())\n","\n","    print (batch_idx, '/', epochs, '|', ' label num :', nLabels, ' | loss :', loss.item())\n","\n","    if nLabels <= minLabels:\n","        print (\"nLabels\", nLabels, \"reached minLabels\", minLabels, \".\")\n","        break"],"metadata":{"id":"3sx2oiyWl_RT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(crop_dataloader, 100, 8)  #Batch size = 32 learning_rate =0.05"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_QSZ0ZKnO-K","outputId":"9a6a6d46-291f-4eed-de13-5c09feba87b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 / 100 |  label num : 350  | loss : 2.932061195373535\n","2 / 100 |  label num : 133  | loss : 2.0080976486206055\n","3 / 100 |  label num : 49  | loss : 1.1151894330978394\n","4 / 100 |  label num : 36  | loss : 0.6813901662826538\n","5 / 100 |  label num : 32  | loss : 0.546470582485199\n","6 / 100 |  label num : 31  | loss : 0.4888755977153778\n","7 / 100 |  label num : 32  | loss : 0.388486385345459\n","8 / 100 |  label num : 32  | loss : 0.4287705421447754\n","9 / 100 |  label num : 32  | loss : 0.3381318151950836\n","10 / 100 |  label num : 30  | loss : 0.33792275190353394\n","11 / 100 |  label num : 30  | loss : 0.29082778096199036\n","12 / 100 |  label num : 32  | loss : 0.2973807454109192\n","13 / 100 |  label num : 32  | loss : 0.26602986454963684\n","14 / 100 |  label num : 30  | loss : 0.26175472140312195\n","15 / 100 |  label num : 31  | loss : 0.24638856947422028\n","16 / 100 |  label num : 32  | loss : 0.2344052791595459\n","17 / 100 |  label num : 30  | loss : 0.2376026213169098\n","18 / 100 |  label num : 30  | loss : 0.22645655274391174\n","19 / 100 |  label num : 30  | loss : 0.24261915683746338\n","20 / 100 |  label num : 37  | loss : 0.48585012555122375\n","21 / 100 |  label num : 33  | loss : 0.38117799162864685\n","22 / 100 |  label num : 30  | loss : 0.22502098977565765\n","23 / 100 |  label num : 32  | loss : 0.19137521088123322\n","24 / 100 |  label num : 32  | loss : 0.19733954966068268\n","25 / 100 |  label num : 30  | loss : 0.17874795198440552\n","26 / 100 |  label num : 30  | loss : 0.2188577800989151\n","27 / 100 |  label num : 30  | loss : 0.17899289727210999\n","28 / 100 |  label num : 30  | loss : 0.18047624826431274\n","29 / 100 |  label num : 30  | loss : 0.16347439587116241\n","30 / 100 |  label num : 30  | loss : 0.167533740401268\n","31 / 100 |  label num : 32  | loss : 0.16192187368869781\n","32 / 100 |  label num : 31  | loss : 0.1846703588962555\n","33 / 100 |  label num : 32  | loss : 0.22895857691764832\n","34 / 100 |  label num : 31  | loss : 0.19367177784442902\n","35 / 100 |  label num : 30  | loss : 0.15611563622951508\n","36 / 100 |  label num : 31  | loss : 0.15716977417469025\n","37 / 100 |  label num : 31  | loss : 0.16509313881397247\n","38 / 100 |  label num : 32  | loss : 0.17388039827346802\n","39 / 100 |  label num : 32  | loss : 0.17234362661838531\n","40 / 100 |  label num : 28  | loss : 0.13947588205337524\n","41 / 100 |  label num : 31  | loss : 0.14465861022472382\n","42 / 100 |  label num : 32  | loss : 0.19834107160568237\n","43 / 100 |  label num : 31  | loss : 0.3605409264564514\n","44 / 100 |  label num : 32  | loss : 0.175967276096344\n","45 / 100 |  label num : 31  | loss : 0.14196518063545227\n","46 / 100 |  label num : 32  | loss : 0.1669469028711319\n","47 / 100 |  label num : 32  | loss : 0.16547659039497375\n","48 / 100 |  label num : 32  | loss : 0.14805613458156586\n","49 / 100 |  label num : 32  | loss : 0.13455131649971008\n","50 / 100 |  label num : 30  | loss : 0.13161103427410126\n","51 / 100 |  label num : 32  | loss : 0.14920824766159058\n","52 / 100 |  label num : 32  | loss : 0.15730315446853638\n","53 / 100 |  label num : 32  | loss : 0.160048708319664\n","54 / 100 |  label num : 32  | loss : 0.12967108190059662\n","55 / 100 |  label num : 30  | loss : 0.12194177508354187\n","56 / 100 |  label num : 30  | loss : 0.11874929815530777\n","57 / 100 |  label num : 32  | loss : 0.14520566165447235\n","58 / 100 |  label num : 30  | loss : 0.11578002572059631\n","59 / 100 |  label num : 30  | loss : 0.1298244297504425\n","60 / 100 |  label num : 32  | loss : 0.11089909076690674\n","61 / 100 |  label num : 30  | loss : 0.1202462911605835\n","62 / 100 |  label num : 29  | loss : 0.3610621392726898\n","63 / 100 |  label num : 30  | loss : 0.11395341157913208\n","64 / 100 |  label num : 30  | loss : 0.1218208521604538\n","65 / 100 |  label num : 32  | loss : 0.1227300763130188\n","66 / 100 |  label num : 32  | loss : 0.21780741214752197\n","67 / 100 |  label num : 32  | loss : 0.11104129254817963\n","68 / 100 |  label num : 30  | loss : 0.1067303717136383\n","69 / 100 |  label num : 32  | loss : 0.11313533782958984\n","70 / 100 |  label num : 30  | loss : 0.11897357553243637\n","71 / 100 |  label num : 32  | loss : 0.26426273584365845\n","72 / 100 |  label num : 30  | loss : 0.1084955558180809\n","73 / 100 |  label num : 30  | loss : 0.10397206991910934\n","74 / 100 |  label num : 32  | loss : 0.11343039572238922\n","75 / 100 |  label num : 32  | loss : 0.1282210499048233\n","76 / 100 |  label num : 32  | loss : 0.10849722474813461\n","77 / 100 |  label num : 32  | loss : 0.11938180774450302\n","78 / 100 |  label num : 32  | loss : 0.11573655903339386\n","79 / 100 |  label num : 32  | loss : 0.1180223673582077\n","80 / 100 |  label num : 32  | loss : 0.12087012082338333\n","81 / 100 |  label num : 32  | loss : 0.10343573987483978\n","82 / 100 |  label num : 32  | loss : 0.12026464194059372\n","83 / 100 |  label num : 32  | loss : 0.10504546016454697\n","84 / 100 |  label num : 32  | loss : 0.09363718330860138\n","85 / 100 |  label num : 30  | loss : 0.08599372208118439\n","86 / 100 |  label num : 30  | loss : 0.09305071830749512\n","87 / 100 |  label num : 32  | loss : 0.09217192232608795\n","88 / 100 |  label num : 30  | loss : 0.08648083359003067\n","89 / 100 |  label num : 29  | loss : 0.0887351930141449\n","90 / 100 |  label num : 32  | loss : 0.13044995069503784\n","91 / 100 |  label num : 31  | loss : 0.18758423626422882\n","92 / 100 |  label num : 32  | loss : 0.1071777194738388\n","93 / 100 |  label num : 30  | loss : 0.3005501925945282\n","94 / 100 |  label num : 30  | loss : 0.09245609492063522\n","95 / 100 |  label num : 32  | loss : 0.10889195650815964\n","96 / 100 |  label num : 32  | loss : 0.09575087577104568\n","97 / 100 |  label num : 32  | loss : 0.08546245843172073\n","98 / 100 |  label num : 31  | loss : 0.09047606587409973\n","99 / 100 |  label num : 32  | loss : 0.0932953953742981\n","100 / 100 |  label num : 32  | loss : 0.09197833389043808\n"]}]},{"cell_type":"code","source":["resnet_18 = resnet18(pretrained=False)\n","resnet_18 = torch.nn.Sequential(*list(resnet_18.children())[:-2])\n","model = resnet_18.train()\n","model = resnet_18.to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","\n","crop_dataloader = DataLoader(crop_dataset, batch_size=16, shuffle=True) "],"metadata":{"id":"7HMOYZH9ach1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(crop_dataloader, 100, 8)  #Batch size = 16 learning_rate =0.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pwVJ_hYEqLmy","outputId":"9e0ec26f-2426-4f55-f3d5-a7ed54cbf659"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 / 100 |  label num : 27  | loss : 0.6706045866012573\n","2 / 100 |  label num : 25  | loss : 0.25471556186676025\n","3 / 100 |  label num : 18  | loss : 0.2398880422115326\n","4 / 100 |  label num : 17  | loss : 0.29093819856643677\n","5 / 100 |  label num : 16  | loss : 0.1495736837387085\n","6 / 100 |  label num : 16  | loss : 0.14788992702960968\n","7 / 100 |  label num : 15  | loss : 0.13239479064941406\n","8 / 100 |  label num : 14  | loss : 0.14970757067203522\n","9 / 100 |  label num : 16  | loss : 0.20074601471424103\n","10 / 100 |  label num : 15  | loss : 0.14906485378742218\n","11 / 100 |  label num : 14  | loss : 0.11265918612480164\n","12 / 100 |  label num : 14  | loss : 0.13614700734615326\n","13 / 100 |  label num : 14  | loss : 0.13541443645954132\n","14 / 100 |  label num : 14  | loss : 0.14484724402427673\n","15 / 100 |  label num : 14  | loss : 0.1350121945142746\n","16 / 100 |  label num : 14  | loss : 0.1564665287733078\n","17 / 100 |  label num : 14  | loss : 0.08787835389375687\n","18 / 100 |  label num : 13  | loss : 0.08566918969154358\n","19 / 100 |  label num : 14  | loss : 0.1288760006427765\n","20 / 100 |  label num : 14  | loss : 0.09909560531377792\n","21 / 100 |  label num : 13  | loss : 0.06711053103208542\n","22 / 100 |  label num : 14  | loss : 0.10386422276496887\n","23 / 100 |  label num : 14  | loss : 0.066294364631176\n","24 / 100 |  label num : 13  | loss : 0.11866991966962814\n","25 / 100 |  label num : 13  | loss : 0.06067197769880295\n","26 / 100 |  label num : 12  | loss : 0.05597218871116638\n","27 / 100 |  label num : 13  | loss : 0.05445780232548714\n","28 / 100 |  label num : 13  | loss : 0.06898826360702515\n","29 / 100 |  label num : 12  | loss : 0.0594814196228981\n","30 / 100 |  label num : 12  | loss : 0.0518004409968853\n","31 / 100 |  label num : 13  | loss : 0.11684688180685043\n","32 / 100 |  label num : 13  | loss : 0.12567688524723053\n","33 / 100 |  label num : 12  | loss : 0.05873309075832367\n","34 / 100 |  label num : 13  | loss : 0.048323165625333786\n","35 / 100 |  label num : 13  | loss : 0.04968191683292389\n","36 / 100 |  label num : 13  | loss : 0.05128762871026993\n","37 / 100 |  label num : 12  | loss : 0.05307087302207947\n","38 / 100 |  label num : 13  | loss : 0.05177712440490723\n","39 / 100 |  label num : 13  | loss : 0.048003438860177994\n","40 / 100 |  label num : 13  | loss : 0.1124349981546402\n","41 / 100 |  label num : 13  | loss : 0.04727090895175934\n","42 / 100 |  label num : 12  | loss : 0.045872244983911514\n","43 / 100 |  label num : 13  | loss : 0.06713826954364777\n","44 / 100 |  label num : 13  | loss : 0.07032664865255356\n","45 / 100 |  label num : 13  | loss : 0.11136818677186966\n","46 / 100 |  label num : 12  | loss : 0.05405048280954361\n","47 / 100 |  label num : 12  | loss : 0.04535367339849472\n","48 / 100 |  label num : 12  | loss : 0.042659975588321686\n","49 / 100 |  label num : 12  | loss : 0.0440567322075367\n","50 / 100 |  label num : 12  | loss : 0.048545826226472855\n","51 / 100 |  label num : 12  | loss : 0.05173933878540993\n","52 / 100 |  label num : 12  | loss : 0.04303428530693054\n","53 / 100 |  label num : 12  | loss : 0.04340384528040886\n","54 / 100 |  label num : 12  | loss : 0.04745565727353096\n","55 / 100 |  label num : 12  | loss : 0.04216400533914566\n","56 / 100 |  label num : 12  | loss : 0.04590243473649025\n","57 / 100 |  label num : 13  | loss : 0.046056877821683884\n","58 / 100 |  label num : 13  | loss : 0.047735795378685\n","59 / 100 |  label num : 12  | loss : 0.04271182045340538\n","60 / 100 |  label num : 12  | loss : 0.04148519039154053\n","61 / 100 |  label num : 12  | loss : 0.042338356375694275\n","62 / 100 |  label num : 12  | loss : 0.04167889803647995\n","63 / 100 |  label num : 12  | loss : 0.041944511234760284\n","64 / 100 |  label num : 12  | loss : 0.04513934627175331\n","65 / 100 |  label num : 12  | loss : 0.040648262947797775\n","66 / 100 |  label num : 12  | loss : 0.04011843353509903\n","67 / 100 |  label num : 12  | loss : 0.04056592285633087\n","68 / 100 |  label num : 12  | loss : 0.044727008789777756\n","69 / 100 |  label num : 12  | loss : 0.04721838980913162\n","70 / 100 |  label num : 13  | loss : 0.059692587703466415\n","71 / 100 |  label num : 12  | loss : 0.04070739448070526\n","72 / 100 |  label num : 12  | loss : 0.04020633548498154\n","73 / 100 |  label num : 12  | loss : 0.04081153869628906\n","74 / 100 |  label num : 13  | loss : 0.06597640365362167\n","75 / 100 |  label num : 12  | loss : 0.039979077875614166\n","76 / 100 |  label num : 13  | loss : 0.048005588352680206\n","77 / 100 |  label num : 13  | loss : 0.042081959545612335\n","78 / 100 |  label num : 12  | loss : 0.04291251301765442\n","79 / 100 |  label num : 13  | loss : 0.06535445898771286\n","80 / 100 |  label num : 13  | loss : 0.06629052758216858\n","81 / 100 |  label num : 12  | loss : 0.049212388694286346\n","82 / 100 |  label num : 12  | loss : 0.04229171574115753\n","83 / 100 |  label num : 12  | loss : 0.04130692034959793\n","84 / 100 |  label num : 12  | loss : 0.039582509547472\n","85 / 100 |  label num : 13  | loss : 0.055403172969818115\n","86 / 100 |  label num : 12  | loss : 0.039230283349752426\n","87 / 100 |  label num : 12  | loss : 0.039110004901885986\n","88 / 100 |  label num : 12  | loss : 0.03892922028899193\n","89 / 100 |  label num : 12  | loss : 0.03941481187939644\n","90 / 100 |  label num : 12  | loss : 0.0410904586315155\n","91 / 100 |  label num : 12  | loss : 0.040767207741737366\n","92 / 100 |  label num : 12  | loss : 0.0401340052485466\n","93 / 100 |  label num : 12  | loss : 0.03910896182060242\n","94 / 100 |  label num : 12  | loss : 0.03904753550887108\n","95 / 100 |  label num : 13  | loss : 0.047808051109313965\n","96 / 100 |  label num : 12  | loss : 0.03997347503900528\n","97 / 100 |  label num : 12  | loss : 0.038737691938877106\n","98 / 100 |  label num : 13  | loss : 0.04147149622440338\n","99 / 100 |  label num : 13  | loss : 0.04546818882226944\n","100 / 100 |  label num : 12  | loss : 0.04215501621365547\n"]}]},{"cell_type":"markdown","source":["# Use Resnet pretrained Weights"],"metadata":{"id":"zSVQ0K2RrLzI"}},{"cell_type":"code","source":["resnet_18 = resnet18(pretrained=True)\n","resnet_18 = torch.nn.Sequential(*list(resnet_18.children())[:-2])\n","\n","model = resnet_18.train()\n","model = resnet_18.to(device)\n","# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","crop_dataloader = DataLoader(crop_dataset, batch_size=16, shuffle=True)"],"metadata":{"id":"hCjQ_as2rPmD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(crop_dataloader, 100, 8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TV26rCpMras5","outputId":"92504930-cf2f-46e7-c7e8-46407e9b8bd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 / 100 |  label num : 36  | loss : 0.377993106842041\n","2 / 100 |  label num : 31  | loss : 0.21667860448360443\n","3 / 100 |  label num : 26  | loss : 0.18946543335914612\n","4 / 100 |  label num : 22  | loss : 0.13776540756225586\n","5 / 100 |  label num : 18  | loss : 0.10034201294183731\n","6 / 100 |  label num : 18  | loss : 0.11784830689430237\n","7 / 100 |  label num : 16  | loss : 0.06966164708137512\n","8 / 100 |  label num : 17  | loss : 0.07500705122947693\n","9 / 100 |  label num : 14  | loss : 0.07832532376050949\n","10 / 100 |  label num : 15  | loss : 0.0581207200884819\n","11 / 100 |  label num : 14  | loss : 0.0528411939740181\n","12 / 100 |  label num : 13  | loss : 0.0680113360285759\n","13 / 100 |  label num : 13  | loss : 0.05786644667387009\n","14 / 100 |  label num : 13  | loss : 0.045792996883392334\n","15 / 100 |  label num : 12  | loss : 0.042237892746925354\n","16 / 100 |  label num : 12  | loss : 0.03820480778813362\n","17 / 100 |  label num : 12  | loss : 0.04540208727121353\n","18 / 100 |  label num : 11  | loss : 0.03934822231531143\n","19 / 100 |  label num : 10  | loss : 0.04209282249212265\n","20 / 100 |  label num : 10  | loss : 0.03654884546995163\n","21 / 100 |  label num : 10  | loss : 0.03327092155814171\n","22 / 100 |  label num : 10  | loss : 0.0408656932413578\n","23 / 100 |  label num : 10  | loss : 0.03755051642656326\n","24 / 100 |  label num : 10  | loss : 0.039472613483667374\n","25 / 100 |  label num : 10  | loss : 0.031130999326705933\n","26 / 100 |  label num : 10  | loss : 0.03646189719438553\n","27 / 100 |  label num : 10  | loss : 0.03529658168554306\n","28 / 100 |  label num : 9  | loss : 0.03749532997608185\n","29 / 100 |  label num : 9  | loss : 0.03390716388821602\n","30 / 100 |  label num : 9  | loss : 0.03448216989636421\n","31 / 100 |  label num : 9  | loss : 0.039559829980134964\n","32 / 100 |  label num : 9  | loss : 0.03428836911916733\n","33 / 100 |  label num : 9  | loss : 0.038151927292346954\n","34 / 100 |  label num : 9  | loss : 0.033599719405174255\n","35 / 100 |  label num : 9  | loss : 0.029775623232126236\n","36 / 100 |  label num : 9  | loss : 0.031011145561933517\n","37 / 100 |  label num : 9  | loss : 0.030242547392845154\n","38 / 100 |  label num : 9  | loss : 0.029263833537697792\n","39 / 100 |  label num : 8  | loss : 0.03168321028351784\n","nLabels 8 reached minLabels 8 .\n"]}]},{"cell_type":"code","source":["def viz(src, dst):\n","    img = cv2.imread(src)\n","    img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n","\n","    label_colours = np.random.randint(255,size=(512,3))\n","\n","    img = torch.from_numpy(img.transpose((2, 0, 1)).astype(\"float32\")) / 255.\n","\n","    img = img.unsqueeze(0)\n","    img = img.to(device=device)\n","    output = model( img )\n","\n","    output = output.data.squeeze(0)\n","    img = img.squeeze(0)\n","    img = img.permute((2, 1, 0))\n","    \n","    output = output.permute( 1, 2, 0 ).contiguous().view( -1, 512 )\n","\n","    print(output.shape)\n","\n","    _, target = torch.max( output, 1 )\n","    im_target = target.data.cpu().numpy()\n","\n","    print(output.data)\n"," \n","    im_target_rgb = np.array([label_colours[ c % 512 ] for c in im_target])\n","\n","    for c in im_target:\n","      print(c)\n","      break\n","\n","    # im_target_rgb = im_target_rgb.reshape( img.shape ).astype( np.uint8 )\n","\n","    \n","    # cv2.imwrite(dst, im_target_rgb)"],"metadata":{"id":"k_3xW8dArbm2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["src = \"/content/drive/MyDrive/Data-20220522T105527Z-002/Rye/scout_point_image_20220318T145503000Z.jpe\"\n","dst = \"/content/output.jpeg\""],"metadata":{"id":"D82JrnvgumMA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["viz(src, dst)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qwayb7tVvgQp","outputId":"831e56ad-e164-4762-fb3d-1ce084a83261"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([49, 512])\n","tensor([[0.4341, 0.4522, 0.4872,  ..., 0.3688, 6.8607, 0.4137],\n","        [0.4177, 0.4575, 0.4587,  ..., 0.4750, 6.3368, 0.3434],\n","        [0.4718, 0.5475, 0.5040,  ..., 0.5492, 0.0000, 0.5337],\n","        ...,\n","        [0.3869, 0.4523, 0.4652,  ..., 0.4530, 0.0000, 0.1902],\n","        [0.4135, 0.5311, 0.4793,  ..., 0.5355, 0.0000, 0.3794],\n","        [0.5319, 0.5521, 0.4868,  ..., 0.5390, 0.0000, 0.4583]],\n","       device='cuda:0')\n","274\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"hOCtAGm4virn"},"execution_count":null,"outputs":[]}]}